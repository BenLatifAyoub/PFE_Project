# config.py
import os

# --- Configuration ---
GROBID_API = "http://localhost:8070/api/processFulltextDocument" # Default GROBID URL
PDFFIGURES_JAR_PATH = "C:/Users/MSI/pdffigures2/pdffigures2.jar" # !!! UPDATE THIS PATH IF NEEDED !!!
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "output")

# --- Database Configuration ---
DB_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': '', # Consider using environment variables or a more secure method for passwords
    'database': 'pfe'
}

# --- Local Model Names (RAG Pipeline) ---
QA_MODEL_NAME = "mrm8488/longformer-base-4096-finetuned-squadv2"
SUM_MODEL_NAME = "allenai/led-large-16384-arxiv" # For summarization agent (if used separately)
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CROSS_ENCODER_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2" # Cross-encoder for re-ranking

# --- RAG Agent Configuration Constants ---
MIN_TEXT_LENGTH = 20         # Minimum characters for a section text to be processed
DEFAULT_TOP_K_RETRIEVAL = 10 # Max sections retrieved by bi-encoder
DEFAULT_TOP_K_RERANK = 5     # Max sections (from retrieval) re-ranked by cross-encoder
DEFAULT_QA_CANDIDATES = 3    # Max sections (after re-ranking/threshold) passed to QA model
DEFAULT_RELEVANCE_THRESHOLD = 0.1 # Min bi-encoder score (if cross-encoder is disabled or as fallback) - Needs Tuning!
DEFAULT_RERANK_THRESHOLD = -100    # Min cross-encoder score for section to be considered relevant for QA - Needs Tuning!
DEFAULT_QA_CONFIDENCE_THRESHOLD = 0.15 # Min confidence score from QA model answer span - Needs Tuning!

# --- Batch Sizes & Processing Parameters (RAG/QA) ---
DEFAULT_EMBEDDING_BATCH_SIZE = 32       # Batch size for sentence-transformer encoding
DEFAULT_CROSS_ENCODER_BATCH_SIZE = 16   # Batch size for cross-encoder predictions
DEFAULT_QA_CONTEXT_TRUNCATION_BUFFER = 50 # Token buffer added when calculating QA context truncation
DEFAULT_QA_MAX_ANSWER_LENGTH = 100      # Max tokens for the *answer* generated by the QA model
DEFAULT_QA_DOC_STRIDE_FACTOR = 0.25     # Overlap factor for QA model context window (e.g., 0.25 -> 25% stride)

# --- Together AI API Configuration ---
DEFAULT_USE_GENERATIVE_AI = True
DEFAULT_GENERATIVE_AI_MODEL_NAME = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
DEFAULT_SUMMARY_REFINEMENT_MODEL_NAME = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY", "742647496d1e8523f9dff118af400f0752ee28459fbc08c5dafe03f8f455de44")
DEFAULT_GENERATIVE_AI_MAX_CONTEXT_TOKENS = 8192
DEFAULT_GENERATIVE_AI_MAX_OUTPUT_TOKENS = 1024
DEFAULT_GENERATIVE_AI_TEMPERATURE = 0.7

# --- ChromaDB Configuration ---
CHROMA_DB_PATH = os.path.join(OUTPUT_DIR, "chroma_db")
CHROMA_COLLECTION_NAME = "article_embeddings_v1"


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyACQwN6IEzGeB59hUvVhGwpWJQiaHr5q9k") 
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest" # Or "gemini-1.5-pro-latest"
# --- DeepL Configuration ---
# DEEPL_AUTH_KEY = os.getenv("DEEPL_AUTH_KEY", None) # <-- REMOVE THIS LINE
# if not DEEPL_AUTH_KEY:                            # <-- REMOVE THIS LINE
#     print("WARNING: DEEPL_AUTH_KEY environment variable not set. Translation agent will be unavailable.") # <-- REMOVE THIS LINE